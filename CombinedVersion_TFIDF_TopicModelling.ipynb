{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "# Cosine \n",
    "from scipy import spatial\n",
    "# KNN & TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data importing\n",
    "def read_trim():\n",
    "    df = pd.read_excel ('koiosDatabase_Concepts_and_Definitions.xlsx')\n",
    "    usefulAttributes = ['CONCEPT_ID', 'TERM_ID', 'DEFINITION_ID' ,'DEFINITION_CONTENT_ID','SYNONYMS_ID', \n",
    "    'CONCEPT_TYPE_ID', 'SYNONYM_VALUE', 'DEFINITION','DEF_FULL_SOURCE_TEXT','TERM_SOURCE_ID']\n",
    "    for x in df.columns.values:\n",
    "        if x not in usefulAttributes:\n",
    "            df.drop(x, 1, inplace=True)  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing a description\n",
    "#return list of \"cleaned\" words\n",
    "def remove_punct_stop(description):\n",
    "    low = [str(t) for t in nlp(description) if t.is_alpha and not t.is_stop]\n",
    "    return low\n",
    "\n",
    "#create a dictionary of terms\n",
    "def term_concepts(df):\n",
    "    # passed as a list of lists of words for gensim bigram\n",
    "    #descriptions = []\n",
    "    \n",
    "    #descriptions = numpy.empty(len(df))\n",
    "    x = 0\n",
    "    \n",
    "    keys = set(df['SYNONYM_VALUE'])\n",
    "    term_to_concepts = dict.fromkeys(keys)\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        term = row['SYNONYM_VALUE']\n",
    "        concept_id = row['CONCEPT_ID']\n",
    "        concept_type = row['CONCEPT_TYPE_ID']\n",
    "        definition = row['DEFINITION']\n",
    "\n",
    "\n",
    "       # descriptions.append(remove_punct_stop(definition))\n",
    "\n",
    "        if term_to_concepts[term] == None:\n",
    "            term_to_concepts[term] = {}\n",
    "\n",
    "        #use concept id as keys for the dictionary of each term\n",
    "        term_to_concepts[term][concept_id] = (concept_type, definition)\n",
    "        \n",
    "    return term_to_concepts, df['DEFINITION'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete terms with only one concept\n",
    "def deletekeys(term_to_concepts):\n",
    "    for x in list(term_to_concepts.keys()):\n",
    "        if len(term_to_concepts[x]) == 1: #len(\n",
    "            del term_to_concepts[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and return the bigram generator\n",
    "def trainBigram(description, min_count, threshold):\n",
    "    phrases = Phrases(description, min_count = min_count, threshold = threshold)\n",
    "    bigram = Phraser(phrases)\n",
    "    return bigram\n",
    "\n",
    "#used for topic modelling\n",
    "def lemmatization(text):\n",
    "    allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    doc = nlp(\" \".join(text))\n",
    "    returnnewtext = [t.lemma_ if t.lemma_ not in ['-PRON-'] \n",
    "                     else '' for t in doc if t.pos_ in allowed_postags]\n",
    "    return returnnewtext\n",
    "\n",
    "#removes stopwords and punctuations from each description\n",
    "#return a list of cleaned descriptions\n",
    "#generate lists of words for tfidf and topic modelling\n",
    "def cleared_text_lists(term_to_concepts, bigram, nr_descriptions):\n",
    "    tfidf_text = [None] * nr_descriptions\n",
    "    topicmodel_text = [[] for _ in range(nr_descriptions)]\n",
    "    \n",
    "    x = 0\n",
    "    for term in term_to_concepts:\n",
    "        for concept in term_to_concepts[term]:\n",
    "            \n",
    "            t, d = term_to_concepts[term][concept]\n",
    "            desc = bigram[remove_punct_stop(d)]\n",
    "\n",
    "            #call lemmatization for the word list used in Topic Modelling\n",
    "            #topicmodel_text.append(lemmatization(desc))\n",
    "            \n",
    "            topicmodel_text[x] = desc\n",
    "            tfidf_text[x]  = ' '.join(desc)\n",
    "    \n",
    "            #tfidf_text.append(tfidf_list)\n",
    "            x = x + 1\n",
    "    return tfidf_text, topicmodel_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for training the tf-idf on all of the descriptions\n",
    "def trainer(l):\n",
    "    tfidf = TfidfVectorizer(ngram_range = (1,1), max_df = 1000, max_features = 30000 )\n",
    "    tfidf.fit(l)\n",
    "    return tfidf\n",
    "\n",
    "#train tfidf on descriptions associated with a single term\n",
    "# def trainer_w_term(term):\n",
    "#     text = []\n",
    "#     for concept in list(term_to_concepts[term]):\n",
    "#             t, d = term_to_concepts[term][concept]\n",
    "#             d = ' '.join([str(t) for t in nlp(d) if not t.is_stop | t.is_punct ])\n",
    "#             text.append(d)\n",
    "#     tfidf = TfidfVectorizer(ngram_range = (1,2))\n",
    "#     tfidf.fit(text)\n",
    "#     return tfidf\n",
    "\n",
    "# Approach I\n",
    "# Nearest Neighbours training method\n",
    "def knn_trainer(tfidf,l):\n",
    "    #samples = []\n",
    "    descriptions = []\n",
    "    \n",
    "    features = numpy.empty(shape=(len(l),20000))\n",
    "    x = 0\n",
    "\n",
    "    for d in l:\n",
    "        v = tfidf.transform([d]).toarray()\n",
    "        features[x] = v[0]\n",
    "#         samples.append(v[0])\n",
    "        x = x+1\n",
    "        del v\n",
    "        descriptions.append(d)\n",
    "        del d\n",
    "\n",
    "   # s = np.array(samples)\n",
    "    print(features.shape)\n",
    "   # del samples\n",
    "    nn = NearestNeighbors(metric = 'cosine')\n",
    "    nn.fit(features)\n",
    "    \n",
    "    return nn, descriptions\n",
    "\n",
    "#return method for KNN\n",
    "def return_top_k(k, d,nn, tfidf, bigram):  \n",
    "    sentence =' '.join(bigram[remove_punct_stop(d)])\n",
    "    v = tfidf.transform([sentence]).toarray()\n",
    "    return nn.kneighbors([v[0]],n_neighbors = k)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RADU'S TFIDF and KNN Pipeline\n",
    "def tfidf_knn(l1, bigram, text,k):\n",
    "    \n",
    "    #call the tfidf trainer method, return a trained tfidf\n",
    "    print(\"STARTED training TF-IDF AT: \",datetime.datetime.now())\n",
    "    tfidf = trainer(l1)\n",
    "    \n",
    "    \n",
    "    print(\"FINISHED training TF-IDF AT: \",datetime.datetime.now())\n",
    "    \n",
    "    #call the knn trainer method, return a trained knn and adjacent list of descriptions\n",
    "    #in order to visualise the results\n",
    "    print(\"STARTED training KNN AT: \", datetime.datetime.now())\n",
    "    knn, ds = knn_trainer(tfidf,l1)\n",
    "    print(\"FINISHED training KNN AT: \",datetime.datetime.now())\n",
    "\n",
    "    #call the method to return the top k -first parameter- most similar\n",
    "    #descriptions to the one passed as the second parameter\n",
    "    print(\"RETURN results for example: \", datetime.datetime.now())\n",
    "    top = return_top_k(5,text, knn, tfidf, bigram)\n",
    "    print(\"Finished at: \", datetime.datetime.now())\n",
    "\n",
    "    #visualise the results\n",
    "    x,pos = top\n",
    "    print(top)\n",
    "    for i in pos[0]:\n",
    "        print(ds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Topic Modelling - LDA Online Training\n",
    "\n",
    "# Create document-term matrix from preprocessed text(tokenize words, remove stopwords, creating bigrams, and word lemmatization)\n",
    "def prepare_corpus(preprocessedDescription):\n",
    "    # Create Dictionary\n",
    "    dictionary = corpora.Dictionary(preprocessedDescription)\n",
    "    \n",
    "    # Create corpus\n",
    "    texts = preprocessedDescription\n",
    "    \n",
    "    # Term Document Frequency\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    return dictionary,corpus\n",
    "\n",
    "# Build the LDA - Online Learning Topic Model\n",
    "def create_lda_model(preprocessedDescription, nr_topics):\n",
    "    dictionary, corpus = prepare_corpus(preprocessedDescription)\n",
    "\n",
    "    # Online LDA training - processes the whole corpus in one pass, then updates the model, then another pass and so on (Faster Execution)\n",
    "    #lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=nr_topics, random_state=100, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=False)\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=nr_topics, random_state=100,passes=10)\n",
    "    # Different methods of printing key topics and associated key words with scores\n",
    "    print(lda_model.print_topics())\n",
    "    #print(lda_model.top_topics(document_term_matrix, dictionary, coherence = 'c_v', topn = words))\n",
    "    #for i, topic in lda_model.show_topics(formatted=True, num_topics = 10, num_words = 5):\n",
    "        #print(str(i) +\": \"+ topic)\n",
    "        #print()\n",
    "\n",
    "    return lda_model\n",
    "\n",
    "# Compute perplexity score and coherence score - performance measurement metrics that give an indication of how good the topic model is\n",
    "def compute_perplexity_coherence(data_lemmatized, lda_model, corpus, dictionary):\n",
    "    print('\\n Perplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\n Coherence Score: ', coherence_lda)\n",
    "\n",
    "# Compute cohrerence scores for multiple trained lda models and compute corresponding coherence scores to determine optimal number of topics\n",
    "def compute_coherence_values(dictionary, corpus, preprocessedDescriptions, limit, start=2, step=3):\n",
    "    # dictionary: Gensim dictionary\n",
    "    # corpus: Gensim corpus\n",
    "    # preprocessedDescriptions: list of processed descriptions\n",
    "    # limit: maximum number of topics\n",
    "    \n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for numberTopics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus, num_topics = numberTopics, id2word = dictionary, passes = 1)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model = model, texts = preprocessedDescriptions, dictionary = dictionary, coherence = 'c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        \n",
    "    return model_list, coherence_values\n",
    "\n",
    "# Plot the above results in the form of a line graph.\n",
    "def plot_graph(preprocessedDescription, start, limit, step):\n",
    "    dictionary, corpus = prepare_corpus(preprocessedDescription)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, corpus, preprocessedDescription, limit, start, step)\n",
    "    \n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x,coherence_values)\n",
    "    #plt.xticks(x)\n",
    "    plt.xlabel(\"Number of topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc = 'best')\n",
    "    plt.show()\n",
    "    \n",
    "    for m, cv in zip(x, coherence_values):\n",
    "        print(\"Number of Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abhi's Topic Modelling using LDA (Online Training)\n",
    "\n",
    "def topic_modelling(l2, nr_topics):\n",
    "    # Method call - prepare_corpus\n",
    "    dictionary, corpus = prepare_corpus(l2)\n",
    "    print(corpus[:1])\n",
    "\n",
    "    # Human readable format of corpus (term-frequency) - Understanding the document-term matrix\n",
    "    [[(dictionary[id], freq) for id, freq in corp] for corp in corpus[:1]]\n",
    "\n",
    "    # Method call - lda model creation and training\n",
    "    lda_model = create_lda_model(l2, nr_topics)\n",
    "\n",
    "    # Method call - perplexity/coherence score computation\n",
    "    compute_perplexity_coherence(l2, lda_model, corpus, dictionary)\n",
    "\n",
    "    # Visualise potential optimum number of topics to determine the diversity of topics and identify closely similar topics, that can be merged into one large topic\n",
    "    lda_model = create_lda_model(l2, nr_topics)\n",
    "    dictionary,corpus = prepare_corpus(l2)\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "    vis\n",
    "    \n",
    "    # Method call - plotting coherence score vs. number of topics\n",
    "    start,limit,step = 2,50,5\n",
    "    plot_graph(l2, start, limit, step) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE I\n",
      "Term-Concept Dictionary is being loaded...\n",
      "Bigram model is being trained...\n",
      "Deleting keys\n",
      "Generate Lists\n",
      "STAGE II\n",
      "STARTED training TF-IDF AT:  2019-07-15 10:08:28.594123\n",
      "FINISHED training TF-IDF AT:  2019-07-15 10:08:28.828562\n",
      "STARTED training KNN AT:  2019-07-15 10:08:28.831556\n",
      "(14546, 40519)\n",
      "FINISHED training KNN AT:  2019-07-15 10:08:44.623082\n",
      "RETURN results for example:  2019-07-15 10:08:44.626075\n",
      "Finished at:  2019-07-15 10:08:51.048194\n",
      "(array([[0.        , 0.06335632, 0.16286706, 0.16641799, 0.17559802]]), array([[ 2378, 12924,  2381,  2380,  2382]], dtype=int64))\n",
      "set_triple line_package outline_styles hole_leads standard_form outline_style described_group data_element types\n",
      "set_triple line_package outline_styles hole_leads outline_style described_group data_element types\n",
      "set_quad line_package outline_styles hole_leads standard_form outline_style described_group data_element types\n",
      "set_dual line_package outline_styles hole_leads standard_form outline_style described_group data_element types\n",
      "set_single line_package outline_styles hole_leads standard_form outline_style described_group data_element types\n",
      "STAGE III\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "topic_modelling() missing 1 required positional argument: 'nr_topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-da8bae014ca6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-da8bae014ca6>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m    \u001b[1;31m#StageIII - Topic Modelling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"STAGE III\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mtopic_modelling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: topic_modelling() missing 1 required positional argument: 'nr_topics'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "   #STAGE I - initialise the dataframes, dictionaries, bigrams and list\n",
    "    print(\"STAGE I\")\n",
    "    df = read_trim()\n",
    "    print(\"Term-Concept Dictionary is being loaded...\")\n",
    "    term_to_concepts, description = term_concepts(df)\n",
    "    print(\"Bigram model is being trained...\")\n",
    "    bigram = trainBigram(description,1,1)\n",
    "    print(\"Deleting keys\")\n",
    "    deletekeys(term_to_concepts)\n",
    "    print(\"Generate Lists\")\n",
    "    l1, l2 = cleared_text_lists(term_to_concepts, bigram)\n",
    "    \n",
    "   #STAGE II - Tfidf\n",
    "    print(\"STAGE II\")\n",
    "    k = 5\n",
    "    text = \"A set of triple-in-line package outline styles with through-hole leads in standard form of which each outline style can be described with the same group of data element types\"\n",
    "    tfidf_knn(l1,bigram,text,k)\n",
    "    \n",
    "   #StageIII - Topic Modelling\n",
    "    print(\"STAGE III\")\n",
    "    topic_modelling(l2)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-18 10:06:20.000512\n",
      "STAGE I\n",
      "Term-Concept Dictionary is being loaded...\n",
      "Bigram model is being trained...\n",
      "Generate Lists\n",
      "2019-07-18 10:16:29.793377\n"
     ]
    }
   ],
   "source": [
    "#STAGE I - initialise the dataframes, dictionaries, bigrams and list\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "print(\"STAGE I\")\n",
    "df = read_trim()\n",
    "print(\"Term-Concept Dictionary is being loaded...\")\n",
    "term_to_concepts, description = term_concepts(df)\n",
    "print(\"Bigram model is being trained...\")\n",
    "bigram = trainBigram(description,1,1)\n",
    "#print(\"Deleting keys\")\n",
    "#deletekeys(term_to_concepts)\n",
    "print(\"Generate Lists\")\n",
    "l1, l2 = cleared_text_lists(term_to_concepts, bigram, len(description))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del term_to_concepts\n",
    "del description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "#APPROACH III\n",
    "\n",
    "#KNN trainer using features from both tfidf and topic modelling\n",
    "def new_knnTrainer(tfidf,l,lda_model, corpus, no_topics):\n",
    "    #descriptions = numpy.empty(shape=(len(l),))\n",
    "    \n",
    "    samples = numpy.empty(shape=(len(l),30050))\n",
    "    x = 0\n",
    "    \n",
    "    for d in l:\n",
    "        tm_features = numpy.zeros(no_topics)\n",
    "        d2 = dictionary.doc2bow(d)\n",
    "        \n",
    "        for tuples in lda_model[d2]:\n",
    "            topic, score = tuples\n",
    "            tm_features[topic] = score\n",
    "        \n",
    "        del d2\n",
    "        \n",
    "        d = ' '.join(t for t in d)\n",
    "        v = tfidf.transform([d]).toarray()\n",
    "        samples[x] = numpy.concatenate((v[0], tm_features), axis=0)\n",
    "        del v\n",
    "        del tm_features\n",
    "        #descriptions[x] = numpy.fromstring(d)\n",
    "        x = x+1\n",
    "    \n",
    "    #s = np.array(samples)\n",
    "    \n",
    "    print(samples.shape)\n",
    "    \n",
    "    nn = NearestNeighbors(algorithm='kd_tree')\n",
    "    nn.fit(samples)\n",
    "    \n",
    "    return nn#, descriptions\n",
    "\n",
    "#Approach II\n",
    "#KNN trainer for topic modelling\n",
    "# numpy array used to store the scores each description has for each topic\n",
    "def KNN_trainer_topicModelling(lda_model, corpus, no_topics):  \n",
    "    no_descriptions = len(corpus)\n",
    "    features = numpy.empty(shape=(no_descriptions,no_topics))\n",
    "    \n",
    "    x = 0\n",
    "\n",
    "    for sentence in corpus:\n",
    "        #print(sentence)\n",
    "        numbers = numpy.zeros(no_topics)\n",
    "        #print(len(numbers))\n",
    "        values = lda_model[sentence]\n",
    "        #print(values)\n",
    "        for tuples in values:\n",
    "            topic, score = tuples\n",
    "            numbers[topic] = score\n",
    "        features[x] = numbers\n",
    "        x = x + 1\n",
    "\n",
    "    #features = scale(features, axis = 1)\n",
    "    nn_topic = NearestNeighbors(algorithm = 'auto',metric = 'cosine')#(metric = 'euclidean')\n",
    "    nn_topic.fit(features)\n",
    "    \n",
    "    return nn_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, corpus = prepare_corpus(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = trainer(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus =corpus, id2word = dictionary, num_topics = 50, random_state = 100, passes = 10)\n",
    "#compute_perplexity_coherence(l2,lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the KNN trainers\n",
    "del nn\n",
    "#nn = (new_knnTrainer(tfidf,l2,lda,corpus,50))\n",
    "#nn = KNN_trainer_topicModelling(lda,corpus,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(lda, new_corpus, dictionary)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.29300081, 0.65108409, 0.66925296, 0.67425509, 0.67746258,\n",
      "        0.68158059, 0.79497501, 0.80258987, 0.86435089, 0.89286119,\n",
      "        0.9006797 , 0.90609412, 0.91167522, 0.91188016, 1.01245985,\n",
      "        1.04067674, 1.04882952, 1.05689774, 1.05787626, 1.05801902]]), array([[54837, 54835,  1333, 54838, 54831, 12470, 30123, 28537, 40548,\n",
      "         1325, 54833,  1328, 12468, 12469,  1329, 40734, 59015, 22997,\n",
      "        61143, 36357]], dtype=int64))\n",
      "set_bead package_outline styles_axial wire_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_cylindrical package_outline styles_axial wire_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_bead package_outline styles_straight wire_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_rectangular package_outline styles_axial wire_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_disc package_outline styles_axial wire_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_bead package_outline styles_formed wire_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_rectangular package_outline styles_axial solder_lugs outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_cylindrical package_outline styles_offset wire_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_rectangular package_outline styles_axial offset solder_lugs outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_rectangular package_outline styles_straight wire_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_cylindrical package_outline styles_radial wire_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_disc package_outline styles_straight wire_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_rectangular package_outline styles_formed wire_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_disc package_outline styles_formed wire_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_cylindrical package_outline styles_straight radial_solder lug_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set hat cylindrical_package outline_styles straight_axial wire_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_industrial products_product described_group data_element types\n",
      "************************************************************************\n",
      "set_dual flat_pack package_outline styles_straight flat_leads outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_cylindrical package_outline styles_upper screw_connections outline_style described_group data_element types\n",
      "************************************************************************\n",
      "set_dual flat_pack package_outline styles flat_leads outline_style described_group data_element types\n",
      "************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#simulate a new text given as input to the KNN model\n",
    "#text = \"A set of triple-in-line package outline styles with through-hole leads in standard form of which each outline style can be described with the same group of data element types\"\n",
    "text = 'A set of bead package outline styles with axial wire leads of wich each outline style can be described with the same group of data element types'\n",
    "text = bigram[remove_punct_stop(text)]\n",
    "\n",
    "text_vec = dictionary.doc2bow(lemmatization(text))\n",
    "sentence = ' '.join(text)\n",
    "v = tfidf.transform([sentence]).toarray()\n",
    "\n",
    "#preprocess the new text\n",
    "processed = numpy.zeros(50)\n",
    "for tuples in lda[text_vec]:\n",
    "    topic, score = tuples\n",
    "    processed[topic] = score\n",
    "\n",
    "#new = [processed]                             \n",
    "new = numpy.concatenate((v[0], processed), axis=0)\n",
    "del v\n",
    "del processed\n",
    "\n",
    "top = nn.kneighbors([new], n_neighbors = 20)#, algorithm = 'auto')    \n",
    "\n",
    "del new\n",
    "\n",
    "x,pos = top\n",
    "print(top)\n",
    "for i in pos[0]:\n",
    "    print(l1[i])\n",
    "#    print(\"l1 version of the sentence: \",l1[i])\n",
    "    print(\"************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return a scipy sparse matrix representing the tfidf weight for each term of each document\n",
    "# # have to convert into corpus to be loaded for lda model\n",
    "\n",
    "# new_l2 = []\n",
    "# for x in l2:\n",
    "#     sentence = ' '.join(x)\n",
    "#     new_l2.append(sentence)\n",
    "\n",
    "\n",
    "# tfidf = trainer(new_l2)\n",
    "# inverse =tfidf.fit_transform(new_l2)\n",
    "# #print(inverse)\n",
    "# new_corpus = []\n",
    "# for row in inverse:\n",
    "#     zipped = zip(row.indices, row.data)\n",
    "#     new_corpus.append(list(zipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
